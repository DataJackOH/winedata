{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8200c60",
   "metadata": {},
   "source": [
    "Lets try and do a few things\n",
    "- are different countries described differently?\n",
    "    - how about different regions?\n",
    "- rating by region\n",
    "- common words by variety\n",
    "- bucket points into bands of ten, most common words in each bucket\n",
    "- most common words by price\n",
    "- most common words low value (points/price < low)\n",
    "- most common words high value\n",
    "- map of top regions\n",
    "- map of most expensive regions\n",
    "\n",
    "\n",
    "EXTRA - \n",
    "can I generate a fake review, region and winery for NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jackohagan/datascience/wine/winemag-data_first150k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1467856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3733b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area'] = df['region_1'] + ', ' + df['province'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unq_address = df['area'].unique()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data['unq_adress'] = unq_address\n",
    "\n",
    "\n",
    "def findGeocode(city):\n",
    "    try: geolocator=Nominatim(user_agent=\"myemail_address@gmail.com\")\n",
    "    except GeocoderTimedOut:\n",
    "        sleep(10)\n",
    "        return findGeocode(city)\n",
    "    return geolocator.geocode(city)\n",
    "\n",
    "data[\"loc\"] = data['unq_adress'].apply(geolocator.geocode)\n",
    "data[\"point\"]= data[\"loc\"].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "df[['lat', 'lon', 'altitude']] = pd.DataFrame(df['point'].to_list(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stop.append('wine')\n",
    "\n",
    "# Exclude stopwords \n",
    "df['description_cleaned'] =  df['country'].astype(str) + ', ' + df['description'].str.lower() \n",
    "df['description_cleaned'] = df['description_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e10b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove punctuation\n",
    "df[\"description_cleaned\"] = df[\"description_cleaned\"].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fc27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a37d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize words (turn into list)\n",
    "\n",
    "\n",
    "df['tokenized_sents']  = df.apply(lambda row: nltk.word_tokenize(row['description_cleaned']), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ded40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lemmatize the tokens - get the 'root' of each word\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "df['lemmatied_description'] = df['tokenized_sents'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##add extra columns, bin into \n",
    "##points / price - value\n",
    "df['value'] = df['points']/df['price']\n",
    "df['price_bucket'] = pd.qcut(df['price'],20,labels=[\"low\", \"lowmid\", \"lowhigh\",'midlow','midmid','midhigh','highlow','highmid','highhigh','10','11','12','13','14','15','16','17','18','19','high'])\n",
    "df['points_bucket'] = pd.cut(df['points'],5, labels=[\"low\", \"midlow\", \"mid\",'midhigh','high'])\n",
    "df['value_bucket'] = pd.qcut(df['value'],20)\n",
    "df.to_csv('tidieddf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799be337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('tidieddf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "def associationfunction(df, column):\n",
    "    \n",
    "\n",
    "\n",
    "    # combine target and description into one list\n",
    "    df['description_cleaned'] =  df[column].astype(str) + ', ' + df['description'].str.lower() \n",
    "    # Exclude stopwords \n",
    "    df['description_cleaned'] = df['description_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    #remove punctuation\n",
    "    df[\"description_cleaned\"] = df[\"description_cleaned\"].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    \n",
    "\n",
    "    ## tokenize words (turn into list)\n",
    "    df['tokenized_sents']  = df.apply(lambda row: nltk.word_tokenize(row['description_cleaned']), axis=1)\n",
    "\n",
    "\n",
    "    ##lemmatize the tokens - get the 'root' of each word\n",
    "    df['lemmatied_description'] = df['tokenized_sents'].apply(lemmatize_text)\n",
    "\n",
    "    \n",
    "    \n",
    "    ##associations \n",
    "    a_list = df['lemmatied_description'].tolist()\n",
    "    ##enconde 1 hot\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(a_list).transform(a_list)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    frequent_itemsets = fpgrowth(df, min_support=0.001, use_colnames=True)\n",
    "    frequent = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\n",
    "    \n",
    "    return frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8dfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass = associationfunction(df=df, column='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba40086",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass[\"antecedent_len\"] = countryass[\"antecedents\"].apply(lambda x: len(x))\n",
    "countryass[\"consequent_len\"] = countryass[\"consequents\"].apply(lambda x: len(x))\n",
    "\n",
    "countryass = countryass[ (countryass['antecedent_len'] == 1) &\n",
    "       (countryass['consequent_len'] ==1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df['country'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53950602",
   "metadata": {},
   "outputs": [],
   "source": [
    "##write file to csv\n",
    "countryass.to_csv('country_similarties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for varietys\n",
    "countryass = associationfunction(df=df, column='variety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfea362",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass[\"antecedent_len\"] = countryass[\"antecedents\"].apply(lambda x: len(x))\n",
    "countryass[\"consequent_len\"] = countryass[\"consequents\"].apply(lambda x: len(x))\n",
    "\n",
    "countryass = countryass[ (countryass['antecedent_len'] == 1) &\n",
    "       (countryass['consequent_len'] ==1) ]\n",
    "\n",
    "##write file to csv\n",
    "countryass.to_csv('variety_similarties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for points\n",
    "countryass= pd.read_csv('variety_similarties.csv')\n",
    "countryass = associationfunction(df=df, column='points_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass[\"antecedent_len\"] = countryass[\"antecedents\"].apply(lambda x: len(x))\n",
    "countryass[\"consequent_len\"] = countryass[\"consequents\"].apply(lambda x: len(x))\n",
    "\n",
    "countryass = countryass[ (countryass['antecedent_len'] == 1) &\n",
    "       (countryass['consequent_len'] ==1) ]\n",
    "\n",
    "##write file to csv\n",
    "countryass.to_csv('points_similarties.csv')\n",
    "#countryass[countryass['antecedents']  == {'low'}].sort_values('lift',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fe1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for points\n",
    "countryass= pd.read_csv('points_similarties.csv')\n",
    "countryass = associationfunction(df=df, column='price_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass[\"antecedent_len\"] = countryass[\"antecedents\"].apply(lambda x: len(x))\n",
    "countryass[\"consequent_len\"] = countryass[\"consequents\"].apply(lambda x: len(x))\n",
    "\n",
    "countryass = countryass[ (countryass['antecedent_len'] == 1) &\n",
    "       (countryass['consequent_len'] ==1) ]\n",
    "\n",
    "##write file to csv\n",
    "countryass.to_csv('price_similarties.csv')\n",
    "#countryass[countryass['antecedents']  == {'low'}].sort_values('lift',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass=pd.read_csv('price_similarties.csv')\n",
    "countryass[countryass['antecedents']  == {'high'}].sort_values('lift',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad875a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryass.sort_values('lift',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
